{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In content-based recommender systems, we use the content information of both users and\n",
    "items while building recommendation engines. A typical content-based recommender\n",
    "system will perform the following steps:\n",
    "1. Generate user profiles.\n",
    "2. Generate item profile.\n",
    "3. Generate the recommendation engine model.\n",
    "4. Suggest the top N recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A profile typically\n",
    "contains preferences for the features of items and users (refer to Chapter 3, Recommendation\n",
    "Engines Explained for details). Once the profiles are created, we choose a method to build the\n",
    "recommendation engine model. Many data-mining techniques such as classification, text\n",
    "similarity approaches such as tf-idf similarity, and Matrix factorization models can be\n",
    "applied for building content-based recommendation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use the multiclass classification approach to build our basic content-based recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step would always be to gather the data and pull it into the programming environment so that we may apply further steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step would be preparing the data required to build the classification models. In this step, we extract the required features of the users and class labels to build the\n",
    "classification model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering why we are choosing binary classification instead of multiclass classification. The choice of model is left to the person building the recommender system; in our case, with the dataset we have chosen, binary class classification fits better than a multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step will be to build the binary classification model. We will choose the RandomForest algorithm to build the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth and final step will be to generate the top-N recommendations for the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item profile creation: In this step, we create a profile for each item using the content information we have about the items. The item profile is usually created using a widely-used information retrieval technique called tf-idf. In Chapter 4,\n",
    "Data Mining Techniques for Recommendation Engines, we explained tf-idf in detail. To recap, the tf-idf value gives the relative importance of features with respect to all the items or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User profile creation: In this step, we take the user activity dataset and preprocess the data into a proper format to create a user profile. We should remember that, in a content-based recommender system, the user profile is created with respect to the item content, that is, we have to extract or compute the preferences of the user for the item content or item features. Usually, a dot product between user activity and item profile gives us the user profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation engine model generation: Now that we have the user profile and item profile in hand, we will proceed to build a recommendation model.\n",
    "Computing a cosine similarity between the user profile and item profile gives us the affinity of the user to each of the items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of the top-N recommendations: In the final step, we shall sort the user-item preferences based on the values calculated in the previous step and\n",
    "then suggest the top-N recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset Description</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the list of packages we will be using for this exercise:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "#Loading the data:\n",
    "path = \"~/anonymous-msweb.test.txt\"\n",
    "\n",
    "raw_data = pd.read_csv(path,header=None,skiprows=7)\n",
    "raw_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the following from the preceding figure:\n",
    "The first column contains three types of values: A/V/C, where A represents case\n",
    "ID, V represents the user, and C represents the case IDs that the user has accessed\n",
    "The second column contains IDs to represent users and items\n",
    "The third column contains the description of website area\n",
    "The fourth contains the URL for the website area on the website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> User Activity </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed toward profile generation, we will have to format the user activity data;\n",
    "the following section explains how to create a user activity dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will create a user-item rating matrix containing users as rows, items as columns, and the value as the cells. Here, the value is either 0 or 1, indicating 1 if the user\n",
    "has accessed the web page, else 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we filter only records that don't contain \"A\" in the first column:\n",
    "user_activity = raw_data.loc[raw_data[0] != \"A\"]\n",
    "\n",
    "# Next, we assign then we remove unwanted columns from the dataset:\n",
    "user_activity = user_activity.loc[:, :1]\n",
    "\n",
    "# Assigning names to the columns of user_activity DataFrame:\n",
    "user_activity.columns = ['category','value']\n",
    "\n",
    "# The following code shows the sample user_activity data:\n",
    "user_activity.head(15)\n",
    "\n",
    "# To get the total unique webid in the dataset, see as the following code:\n",
    "len(user_activity.loc[user_activity['category'] ==\"V\"].value.unique())\n",
    "\n",
    "# To get the unique users count, see following code:\n",
    "len(user_activity.loc[user_activity['category'] ==\"C\"].value.unique())\n",
    "\n",
    "# Now let's run the following code to create a user-item-rating matrix, as follows:\n",
    "\n",
    "# First, we assign variables:\n",
    "tmp = 0\n",
    "nextrow = False\n",
    "\n",
    "#Then we get the last index of the dataset:\n",
    "lastindex = user_activity.index[len(user_activity)-1]\n",
    "lastindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The for loop code loops through each record and adds new columns('userid', 'webid') to user_activity data frame which shows userid and corresponding web activity:\n",
    "for index,row in user_activity.iterrows():\n",
    "    if(index <= lastindex ):\n",
    "        if(user_activity.loc[index,'category'] == \"C\"):\n",
    "            tmp = 0\n",
    "            userid = user_activity.loc[index,'value']\n",
    "            user_activity.loc[index,'userid'] = userid\n",
    "            user_activity.loc[index,'webid'] = userid\n",
    "            tmp = userid\n",
    "            nextrow = True\n",
    "            \n",
    "        elif(user_activity.loc[index,'category'] != \"C\" and nextrow == True):\n",
    "            webid = user_activity.loc[index,'value']\n",
    "            user_activity.loc[index,'webid'] = webid\n",
    "            user_activity.loc[index,'userid'] = tmp\n",
    "            \n",
    "            if(index != lastindex and user_activity.loc[index+1,'category'] == \"C\"):\n",
    "                nextrow = False\n",
    "                caseid = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we remove the unwanted rows from the preceding data frame, that is, we will be removing the rows containing \"C\" in the category column:\n",
    "user_activity = user_activity[user_activity['category'] == \"V\" ]\n",
    "\n",
    "#We subset the columns, and remove the first two columns, which we no longer needed:\n",
    "user_activity = user_activity[['userid','webid']]\n",
    "\n",
    "# Next, we sort the data by webid; this is to make sure that the rating matrix generation is in good format:\n",
    "user_activity_sort = user_activity.sort('webid', ascending=True)\n",
    "\n",
    "# Now, let's create a dense binary rating matrix containing user_item_rating using the following code:\n",
    "\n",
    "# First, we get the size of webid column:\n",
    "sLength = len(user_activity_sort['webid'])\n",
    "\n",
    "# Then we add a new column, 'rating' to the user_activity data frame which contains only 1:\n",
    "user_activity_sort['rating'] = pd.Series(np.ones((sLength,)), index=user_activity.index)\n",
    "\n",
    "# Next, we use pivot to create binary rating matrix:\n",
    "ratmat = user_activity_sort.pivot(index='userid', columns='webid', values='rating').fillna(0)\n",
    "\n",
    "# Finally, we create a dense matrix:\n",
    "ratmat = ratmat.to_dense().as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Item profile generation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create item data, we will consider the data that contains A in the first column:\n",
    "    \n",
    "#First, we filter all the records containing first column as \"A\"\n",
    "items = raw_data.loc[raw_data[0] == \"A\"]\n",
    "\n",
    "# Then we name the columns as follows:\n",
    "items.columns = ['record','webid','vote','desc','url']\n",
    "\n",
    "# To generate item profile we only needed two columns so we slice the dataframe as follows:\n",
    "items = items[['webid','desc']]\n",
    "\n",
    "# To see the dimensions of the items, the dataframe is given like, We observe that there are 294 unique webid in the dataset:\n",
    "items.shape\n",
    "\n",
    "# To check the sample of the data, we use the following code:\n",
    "Items.head()\n",
    "\n",
    "# To check the count of unique webid, we use the following code:\n",
    "items['webid'].unique().shape[0]\n",
    "\n",
    "# We can also only those webid which are present in the user_activity data:\n",
    "items2 = items[items['webid'].isin(user_activity['webid'].tolist())]\n",
    "\n",
    "# We can use the following code check type of the object\n",
    "type(items2)\n",
    "\n",
    "# We can also sort the data by webid:\n",
    "items_sort = items2.sort('webid', ascending=True)\n",
    "\n",
    "# Let'see what we have done till now, using the head(5) function:\n",
    "items_sort.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall create the item profile using the tf-idf functions available in the sklearn package. To generate tf-idf, we use the TfidfVectorizer(). The fit_transform()\n",
    "methods are in the sklearn package. The following code shows how we can create tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following code, the choice of the number of features to be included depends on the dataset, and the optimal number of features can be selected by the cross-validation approach:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer(stop_words =\"english\",max_features = 100,ngram_range=(0,3),sublinear_tf =True)\n",
    "x = v.fit_transform(items_sort['desc'])\n",
    "itemprof = x.todense()\n",
    "\n",
    "itemprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> User Profile Creation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have item profile and user activity in hand; the dot product between these two matrices will create a new matrix with dimensions equal to # of users by # Item features.\n",
    "To compute the dot product between user activity and item profile, we use the scipy package methods such as linalg, dot available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code to compute the dot product:\n",
    "\n",
    "#user profile creation\n",
    "from scipy import linalg, dot\n",
    "userprof = dot(ratmat,itemprof)/linalg.norm(ratmat)/linalg.norm(itemprof)\n",
    "userprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Recommendation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in a recommendation engine model would be to compute the active user preferences for the items. For this, we do a cosine similarity between user profile and item\n",
    "profile.\n",
    "To compute the cosine calculations, we will be using the sklearn package. The following code will calculate the cosine_similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the cosine similarity between userprofile an item profile:\n",
    "\n",
    "import sklearn.metrics\n",
    "similarityCalc = sklearn.metrics.pairwise.cosine_similarity(userprof, itemprof, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the results of the preceding calculation as follows:\n",
    "similarityCalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's format the preceding results calculated as binary data (0,1), as follows:\n",
    "\n",
    "# First, we convert the rating to binary format:\n",
    "final_pred= np.where(similarityCalc>0.6, 1, 0)\n",
    "\n",
    "# Then we examine the final predictions of first three users:\n",
    "final_pred[1]\n",
    "final_pred[2]\n",
    "final_pred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the zero values from the preceding results gives us the list of the probable items that can be recommended to the users:\n",
    "\n",
    "# For user 213 the recommended items are generated as follows:\n",
    "indexes_of_user = np.where(final_pred[213] == 1)\n",
    "indexes_of_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
